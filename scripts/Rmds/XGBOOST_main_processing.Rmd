---
title: "XGboost main processing"
output:
  html_document:
    self_contained: false
knit: (function(inputFile, encoding) {
  rmarkdown::render(inputFile, encoding = encoding, output_dir = here::here("output/"))})
---

```{r setup, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load data and Preprocessing

Can be examined on data_load_and_preprocess.Rmd/html auxillary notebook, here it is only loaded:

```{r, results='hide'}
source(here::here("scripts","Rmds","data_load_and_preprocess.R"))
```

#### XGBoost adaptations

Load feature selected by "pamr":

```{r}
selected_genes=read.csv(here("output/selected_genes.csv"))
```

Additional formatting to adapt to format request of XGBoost algorithm, subsetting to selected genes by "pamr" and adding "Alignment Quality Index" as covariate:

traindata-ynum integer (0,1,2) corresponds to the levels (LUAD,LUSC,NORM) of traindata-y

```{r}
traindata$ynum=as.integer(traindata$y) - 1
traindata$x=t(traindata$x[selected_genes$sgenes_indexTh,])
traindata$x=cbind(traindata$x,traindata$covariates$`Alignment Quality Index`) #add AQI to index
colnames(traindata$x)[ncol(traindata$x)] <- 'Sequencing Quality Index'

testdata$ynum=as.integer(testdata$y) - 1
testdata$x=t(testdata$x[selected_genes$sgenes_indexTh,])
testdata$x=cbind(testdata$x,testdata$covariates$`Alignment Quality Index`) #add AQI to index
colnames(testdata$x)[ncol(testdata$x)] <- 'Sequencing Quality Index'

numclass=length(unique(traindata$ynum)) #needed by XGBoost input
```

```{r} 
print(paste0("Nr. training obvs: ", length(traindata$y))) #code not included in output
print(paste0("Nr. test obvs: ", length(testdata$y)))
print((paste0("Nr. of features (genes) selected: ", (ncol(traindata$x)))))
```


# Fitting

```{r xgboostfitting, results='hide', warning=FALSE} 
library(xgboost)

xgb_model <- xgboost(data = traindata$x, label=traindata$ynum, num_class=numclass,
                     nrounds = 100, 
                     eta=0.1,
                     gamma=0.1,
                     max_depth=3,
                     colsample_bytree=0.4,
                     min_child_weight=3,
                     objective="multi:softprob",verbose = FALSE)
```

# Visualizing XGB Classifications of Train and Test cases

Preparing the quantities for them through the flexible function \texttt{vcr.custom.*}

```{r, include=FALSE}
library(caret) #for a detailed confusion matrix
library(classmap) #the classmap package
library(classmapExt) #the extension package with supplementary functions
```

### Silplot and Quasi-residual plots

No particular computations needed, just compute and feed the posteriors.

Posteriors for both train and test set:

```{r}
traindata$posteriors = predict(xgb_model,traindata$x,reshape=T)
colnames(traindata$posteriors) = levels(traindata$y)
traindata$ypred = apply(traindata$posteriors,1,function(x) colnames(traindata$posteriors)[which.max(x)])

testdata$posteriors = predict(xgb_model,testdata$x,reshape=T)
colnames(testdata$posteriors) = levels(traindata$y)
testdata$ypred = apply(testdata$posteriors,1,function(x) colnames(testdata$posteriors)[which.max(x)])
```

Using the flexible function to just produce PAC and Sil values

```{r}
#feeding only true labels and posteriors
vcrtrain=vcr.custom.train(traindata$y, probs=traindata$posteriors) 
vcrtest=vcr.custom.newdata(testdata$y, probs=testdata$posteriors, vcr.custom.train.out=vcrtrain)
```

Silplots:

```{r, dev='svg'}
confusionMatrix(factor(traindata$ypred), traindata$y) #can also put vcrtrain$ypred
silplot(vcrtrain)
confusionMatrix(factor(testdata$ypred), testdata$y)
silplot(vcrtest)
```

Qresplots (only produced for continuos covariates in traning set):

```{r, fig.height=8, fig.width=10, dev='svg'}

par(mfrow=c(2,3))

qresplot(vcrtrain$PAC,traindata$pancovariates$MSI.MANTIS.Score, plotErrorBars = TRUE, 
         main="Mantis Score", xlim=c(0.2,0.4))
qresplot(vcrtrain$PAC,traindata$covariates$years_smoked, plotErrorBars = TRUE, 
         main="Years smoked")
qresplot(vcrtrain$PAC,traindata$covariates$longest_dimension, plotErrorBars = TRUE, 
         main="Tumor dimension (longest)", xlim=c(0,3))
qresplot(vcrtrain$PAC,traindata$pancovariates$Fraction.Genome.Altered, plotErrorBars = TRUE, 
         main="Fraction of Genome Altered")
qresplot(vcrtrain$PAC,traindata$pancovariates$Diagnosis.Age, plotErrorBars = TRUE, 
         main="Age")
qresplot(vcrtrain$PAC,traindata$covariates$`Alignment Quality Index`, plotErrorBars = TRUE,
         main="Alignment Quality Index", xlim = c(0.045,0.11), grid=seq(0.04, 0.11, length.out=10))

```

### Classmaps and MDS color-scaled plots

Here we need some additional compututations of some quantities.

To enable the classmap plot we should devise a proper way to measure the distance of each observation $i$ to each given class $g$ (D($i$,$g$)) according to the trained classifier view on the data. We should produce a "distance to class' matrix for all observations in the train set and another for the ones in the test set. The matrices produced will be respectly feeded in `vcr.custom.train` and `vcr.custom.newdata` and will allow for `Farness` computation.

To produce D($i$,$g$) for our XGBoost model, which belongs to the large family of tree classifiers, we essentially build upon the same concept that Rousseuw and Raymakers established for basic classification tree algorithms (Supplementary Material section A.3 of *Silhouttes and Quasi Residual Plots For Neural Nets and Tree-based Classifiers 2022 by Raymaekers and ROusseeuw*).

To do that we start by computing pairwise dissimilarities between the points in the training set by the *gower* metric (here essentially a weighted euclidean since all feature are numeric weighted for the average importance of the features in the classifier as outputted in the Gain column after calling `xgb.importance` on our model:

```{r}
xgb.importance(model=xgb_model)

traindata$importance=xgb.importance(model=xgb_model)
traindata$weight=rep(0,ncol(traindata$x)) #contains weight for all genes 
                                          #in proper order
names(traindata$weight)=colnames(traindata$x)
for (i in 1:nrow(traindata$importance)) {
  traindata$weight[as.character(traindata$importance[i,1])]=
  as.numeric(traindata$importance[i,2])
}

feature_importance_xgb=stack(traindata[["weight"]])
write.csv(feature_importance_xgb, here::here("output/feature_importance_xgb.csv"), row.names = FALSE)
```

```{r}
library(cluster)
traindata$pwd=as.matrix(daisy(traindata$x, metric="gower", weights = traindata$weight))
any(is.na(traindata$pwd)) #check if there is any NAs (could happen if a weight is exactly zero)

#we compute the pairwise dissimilarities also among observations of test: 
#(will be needed later for mdscolorscale plot:)

testdata$pwd=as.matrix(daisy(testdata$x, metric="gower", weights = traindata$weight))
any(is.na(testdata$pwd)) #check if there is any NAs (could happen if a weight is exactly zero)
```

Now for the training set we compute the 'distance to class' matrix. We take, as the distance of a generic observation $i$ to a generic class $g$, the median among the $k=5$ smaller dissimilarities (because of locality nature of decision space of tree algorithms) between object $i$ and the set of objects $j$ actually belonging to class $g$.

```{r}
# Compute neighbors by sorting dissimilarities:
sortNgb <- t(apply(traindata$pwd, 1, order))[, -1] #contains indexes of nearest 
                                                   #points for each row 
sortDis <- t(apply(traindata$pwd, 1, sort))[, -1]  #contains dimmilarieties  
                                                   #values sorted
k=5 #neighbor to consider
yintv=as.numeric(traindata$y)
#create empty structure to fill:
traindata$distToClass <- matrix(rep(NA, nrow(traindata$x) * numclass), ncol = numclass) 
for (i in seq_len(nrow(traindata$x))) { #loop over all cases
  for (g in seq_len(numclass)) { #loop over classes
    ngbg <- which(yintv[sortNgb[i, ]] == g) #getting indexes of all in the same class
    if (length(ngbg) > k) {ngbg <- ngbg[seq_len(k)]} #getting the k nearer
    traindata$distToClass[i, g] <- median(sortDis[i, ngbg]) #take the median of the k nearer
  }
}
```

Ultimately, to compute the 'distance to class' matrix for the cases in the test set we should only use the given considered case and the information from the train set. So each test case \eqn{t} is taken separately and the pairwise distances are calculated on the set composed by the training observation plus the test case considered. Then the distance between the \eqn{t} case and the generic class \eqn{g} is computed exactly like before, that is by taking the median among the k=5 smaller dissimilarities between \eqn{t} and the set of training objects \eqn{j} actually belonging to class \eqn{g}. (running this chunk may take a while cause of the nested loops)

```{r}

testdata$newDistToClass <- matrix(rep(NA, nrow(testdata$x) * numclass), ncol = numclass)

for (i in 1:nrow(testdata$x)){
  testtrain=rbind(testdata$x[i,], traindata$x)
  yintv=rbind(as.numeric(testdata$label)[i],as.numeric(traindata$y)) #add also true int label of   test for structure consistencies (anyway after will be not considered)
  testtrainpwd=as.matrix(daisy(testtrain, metric="gower", weights = traindata$weight))
  sortNgb <- t(apply(testtrainpwd, 1, order))[1, -1] #taking first row we take the i test obvs
  sortDis <- t(apply(testtrainpwd, 1, sort))[1, -1] 
  
  for (g in seq_len(3)) { # loop over classes
    ngbg <- which(yintv[sortNgb] == g) #getting indexes of all in the considered
    if (length(ngbg) > k) {ngbg <- ngbg[seq_len(k)]} #getting the k nearer
    testdata$newDistToClass[i,g] <- median(sortDis[ngbg]) #take the median of the k nearer
  }
}
```

Now it is possible to run `vcr.custom.train/test` inputting also `distToClass` and allow for `farness` computation that in turn allow for `classmap` plots.

```{r, out.width='120%'}
vcrtrain=vcr.custom.train(traindata$y, traindata$posteriors, distToClasses = traindata$distToClass)
 vcrtest=vcr.custom.newdata(ynew = testdata$y , probs = testdata$posteriors , 
                           vcr.custom.train.out = vcrtrain, newDistToClasses = testdata$newDistToClass)
```

Classmaps:

```{r , fig.height=3.5, dev='svg'}
par(mfrow=c(1,3))
classmap(vcrtrain, whichclass = 1, main = "Pred of class LUAD" )
classmap(vcrtrain, whichclass = 2, main = "Pred of class LUSC")
classmap(vcrtrain, whichclass = 3, main = "Pred of class NORM")
mtext(paste0("Classmaps for train set XGB"), line=-5, side=3, outer=TRUE, cex=1)
```

```{r , fig.height=3.5, dev='svg'}
par(mfrow=c(1,3))
classmap(vcrtest, whichclass = 1, main = "Pred of class LUAD" )
classmap(vcrtest, whichclass = 2, main = "Pred of class LUSC")
classmap(vcrtest, whichclass = 3, main = "Pred of class NORM")
mtext(paste0("Classmaps for test set XGB"), line=-5, side=3, outer=TRUE, cex=1)
```

MDS color-scaled plot feeding already computed pairwise dissimilarities:

```{r}
mdscolorscale(vcrtrain, diss=traindata$pwd, main="(Train) MDScolorscale XGB")
```

```{r}
mdscolorscale(vcrtest, diss=testdata$pwd, main="(Test) MDScolorscale XGB")
```

